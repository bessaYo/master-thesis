{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=3,\n",
        "            stride=stride, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels, out_channels, kernel_size=3,\n",
        "            stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Identity()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels, out_channels,\n",
        "                    kernel_size=1, stride=stride, bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = 16\n",
        "        channels = [16, 32, 64]\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, 16, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, channels[0], num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, channels[1], num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, channels[2], num_blocks[2], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[-1], num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        layers = [block(self.in_channels, out_channels, stride)]\n",
        "        self.in_channels = out_channels\n",
        "\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2, 2, 2], num_classes)\n",
        "\n",
        "\n",
        "def ResNet34(num_classes=10):\n",
        "    return ResNet(BasicBlock, [6, 6, 6], num_classes)\n",
        "\n",
        "\n",
        "def ResNet50(num_classes=10):\n",
        "    return ResNet(BasicBlock, [9, 9, 9], num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=(0.4914, 0.4822, 0.4465),\n",
        "        std=(0.2023, 0.1994, 0.2010)\n",
        "    )\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=(0.4914, 0.4822, 0.4465),\n",
        "        std=(0.2023, 0.1994, 0.2010)\n",
        "    )\n",
        "])\n",
        "\n",
        "train_set = datasets.CIFAR10(\"data\", train=True, download=True, transform=transform_train)\n",
        "test_set  = datasets.CIFAR10(\"data\", train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in tqdm(loader, leave=False):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        pred = model(x).argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return 100.0 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ResNet18().to(device)\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.1,\n",
        "    momentum=0.9,\n",
        "    weight_decay=5e-4\n",
        ")\n",
        "\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer,\n",
        "    milestones=[100, 150],\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "EPOCHS = 150\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    acc = evaluate(model, test_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\n",
        "        f\"[ResNet18] Epoch {epoch+1:03d} | \"\n",
        "        f\"Loss {loss:.3f} | Acc {acc:.2f}%\"\n",
        "    )\n",
        "\n",
        "checkpoint = {\n",
        "    \"model\": \"ResNet18-CIFAR10\",\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"test_acc\": acc,\n",
        "    \"state_dict\": model.state_dict()\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, \"resnet18_cifar10.pt\")\n",
        "print(\"Saved resnet18_cifar10.pt\")\n",
        "files.download(\"resnet18_cifar10.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ResNet34().to(device)\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.1,\n",
        "    momentum=0.9,\n",
        "    weight_decay=5e-4\n",
        ")\n",
        "\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer,\n",
        "    milestones=[100, 150],\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "EPOCHS = 150\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    acc = evaluate(model, test_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\n",
        "        f\"[ResNet34] Epoch {epoch+1:03d} | \"\n",
        "        f\"Loss {loss:.3f} | Acc {acc:.2f}%\"\n",
        "    )\n",
        "\n",
        "checkpoint = {\n",
        "    \"model\": \"ResNet34-CIFAR10\",\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"test_acc\": acc,\n",
        "    \"state_dict\": model.state_dict()\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, \"resnet34_cifar10.pt\")\n",
        "print(\"Saved resnet34_cifar10.pt\")\n",
        "files.download(\"resnet34_cifar10.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ResNet50().to(device)\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.1,\n",
        "    momentum=0.9,\n",
        "    weight_decay=5e-4\n",
        ")\n",
        "\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer,\n",
        "    milestones=[100, 150],\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "EPOCHS = 150\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    acc = evaluate(model, test_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\n",
        "        f\"[ResNet50] Epoch {epoch+1:03d} | \"\n",
        "        f\"Loss {loss:.3f} | Acc {acc:.2f}%\"\n",
        "    )\n",
        "\n",
        "checkpoint = {\n",
        "    \"model\": \"ResNet50-CIFAR10\",\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"test_acc\": acc,\n",
        "    \"state_dict\": model.state_dict()\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, \"resnet50_cifar10.pt\")\n",
        "print(\"Saved resnet50_cifar10.pt\")\n",
        "files.download(\"resnet50_cifar10.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
